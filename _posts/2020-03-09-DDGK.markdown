---
title: Learning Graph Representations for Deep Divergence Graph Kernels (WWW 2019)
date: 2020-03-09 00:00:00 +0000
categories: [Paper Review]
tags: [Paper Review, Graph Neural Network, Graph Classification]
---
<br/>  

지난 번 살펴봤던 GCN이 수행한 Task는 하나의 그래프 네트워크에서 각 노드의 Class를 맞추는 Node Classification Task였다. 이외에도 그래프를 재구성하여 missing edge를 찾는 link prediction Task, 그래프 자체의 클래스를 맞추는 Graph Classification Task 등이 있는데, 요번에 살펴본 논문은 GNN 뿐 아니라 그 이전에 리뷰했던 오토인코더의 개념도 활용하여 Unsupervised하게 그래프 분류 문제를 해결할 수 있는 방법을 제안한다. 

## <b>Graph Kernel</b>  
논문에서 제안된 <b>Deep Divergence Graph Kernel를</b> 본격적으로 보기 앞서 Graph Kernel이 무엇인지 보고 가는 것이 좋을 것 같다. 위키피디아에 의하면 그래프 커널은 쌍으로 이루어진 그래프들의 유사도를 비교하기 위한 inner product function이라고 한다. 그래프 커널은 1999년에 convolutional graph kernel이라는 컨셉으로 처음 등장한 아직은 어린(?) 이론이라고 볼 수 있는데, 논문을 읽어보려다가 전공 원서를 읽는 기분이 들어 우선 덮어 두었다.  
<br/>
<img src="/assets/img/pr/ddgk/ddgkone.jpg">  
<br/>
복잡한 식들을 보기에 앞서 <b>유사한 그래프란</b> 무엇인가에 대해 이해할 필요가 있다. 결론부터 말하자면 위 사진의 두 그래프는 유사하다. 정확히 말하면 두 그래프는 <b>isomorphic</b>하다. Graph Isomorpism이란 하나의 그래프에서의 연결관계를 어떤 함수를 통해 다른 그래프에서 똑같이 구현해 낼 수 있음을 말한다. 재미없게 말하자면 <b>(x,y) is an edge of G1 iff (f(x),f(y)) is an edge of G2. Then f is an isomorphism, and G1 and G2 are called isomorphic</b> 이라고 한다. 그런데 이 함수 f를 찾는 일은 NP-complete(아주 어려운 일)이다.  
<br/>  
<img src="/assets/img/pr/ddgk/ddgktwo.jpg">  
<br/>
그리고 이런 복잡한 문제를 해결하기 위한 여러 kernel method들이 고안됬고, 딥러닝의 관점에서 두 그래프가 얼마나 Isomorphinc 한지 계산하고자 시도한 것이 해당 논문이라 볼 수 있다. 사실 논문의 여러 실험에서 비교 대상이 된 Weisfeiler-Lehman Kernel(2011)에 대해 리뷰해보려고 했는데, 쉽게 풀어놓은 리뷰도 별로 없고 논문의 무시무시함이 파괴적이라 구체적인 이해는 나아아중으로 미루고자 한다.  
  
## <b>Novelty</b>
개인적으로는 제목 다음으로는 저자가 자랑하고자 하는 것들을 요약해 놓은 부분을 먼저 읽는 편이다.  
<br/>
<b>• Deep Divergence Graph Kernels:</b> A novel method for
learning unsupervised representations of graphs and their
nodes. Our kernel is learnable and does not depend on feature engineering or domain knowledge.  
  
<b>• Isomorphism Attention:</b> A cross-graph attention mechanism to probabilisticly align representations of nodes between graph pairs. These attention networks allow for great
interpretablity of graph structure and discoverablilty of similar substructures.  
  
<b>• Experimental results:</b> We show that DDGK both encodes
graph structure to distinguish families of graphs, and when
used as features, the learned representations achieve competitive results on challenging graph classification problems
like predicting the functional roles of proteins.  
  
요약하자면, #Unsupervised #Cross Graph Attention #Good  
그럼 본격적으로 모델의 구조를 뜯어보자.  
<br/>

## <b>DDGK</b>  
<img src="/assets/img/pr/ddgk/ddgkthree.jpg">  
<br/>
왼쪽부터 보는 것이 마음은 편하지만, 특이한 DDGK의 학습 방법 때문에 가장 먼저 보아야할 것은 초록색 네모 박스로 되어있는 <b>Source Graph Encoder</b> 부분을 봐야한다. 이름에서 유추할 수 있듯이 이부부은 Input과 동일한 차원의 Dimension을 내뱉는 일반적인 오토인코더의 형태를 하고 있다. 다만 특이한 점은 Reconstruction의 목표가 되는 Output이 Input 그 자체가 아니라 Input에 연결된 다른 노드 정보라는 것이다.즉, 위의 사진의 target graph를 생각해봤을 때, 1번 노드의 값이 오토인코더에 input으로 들어갔을 때, 2와 6이 output으로 나오도록 학습이 진행될 것이다.  
<br/>  
<img src="/assets/img/pr/ddgk/ddgkfour.jpg">  
<br/>
구체적인 
