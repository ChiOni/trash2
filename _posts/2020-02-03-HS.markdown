---
title: Improving regression performance with distributional losses
date: 2020-02-03 00:00:00 +0000
catagories: [Paper Review]
tags: [Paper Review, Distiributional Losses]
---

<br/>
Improving regression performance with distributional losses(ICML 2018)  
  
기존의 0/1 라벨이 아닌 Soft Target을 사용하는 것이 여러 분류 문제의 성능 향상에 기여한다는 것이 정설이 되어가고 있다. Soft Target을 사용하는 방법에는 단순히 노이즈를 추가하거나, Knowledge Distillation을 사용하는 등의 방법이 있다. 특히 이미 강화학습 분야에서는 모델이 분포를 학습하는 것이 왜 성능을 향상시키는가에 대한 증거가 존재한다. 해당 논문에서는 이것을 일반화하여 범용적인 Regression 문제에서 적용가능한 새로운 형태의 Loss를 제안한다. 또한 단순히 제안과 실험에서 끝내는 것이 아니라 성능 향상의 이유를 수학적으로 추론한 부분이 흥미롭다.  

<br/>

## <b>Paper Summary</b>  
1. Distributional Losses를 통해 Regression 문제의 예측 성능을 향상시킬 수 있다.  
  
2. 우리는 해당 로스가 왜 더 좋은 성능을 발휘하는가에 대해 수학적으로 추론했다.  
  
3. 논문에서 제안된 Distributional Losses = Histogram Loss는 계산이 용이하다.  

<br/>

<br/>

## <b>Histogram Loss</b>  
논문에서 제안한 Histogram Loss(HS)는 input x에 대한 ouput y의 분포를 예측하도록 학습된다. 그리고 "True" y 분포와 "Predict" y 분포간의 KL-Divergence의 크기가 Loss가 된다. 처음에는 Output의 형태가 직관적으로 와닿지 않을 수 있지만 로스의 이름이 Histogram이라는 것을 기억하자. 우리가 학습할 모델 f는 히스토그램을 아웃풋으로 내놓게 된다. 쉽게 말하자면, 히스토그램의 막대 갯수만큼의 숫자를 내놓게 된다. 정확히 말하자면, 합이 1이며 [0,1] 사이의 k(# of bins) 길이의 벡터를 내놓게 된다.  
<br/>

<br/> 
<img src="/assets/img/pr/hs/hsone.jpg">
<br/>  
불행하게도 논문에서는 직관적으로 이것을 이해시켜줄 어떤 그림도 없다. 저자의 머릿속에 있었을 그림이 아마 저 위의 Plot일 것이라 추측하는데, Loss가 목적하는 것은 연속적인 "True" 분포에 각 원소가 density를 의미하는 k 길이의 벡터가 딱 들어맞는 것이다. 이것을 KL-Divergece를 사용하여 표현하면 아래와 같다.  

<br/> 

𝐷_𝐾𝐿 (𝑃//𝑄)= ∑_(𝑥∈𝑋)〖𝑝(𝑥)∗log⁡((𝑄(𝑥))/(𝑃(𝑥)))〗  

= ∑〖𝑝(𝑥)∗log⁡(𝑞(𝑥))〗− ∑〖𝑝(𝑥)∗log⁡(𝑝(𝑥))〗  

= 𝐻(𝑃,𝑄) − 𝐻(𝑃)  

<br/> 
<b style="text-align: center;">우리는 Q에 대한 Gradient를 사용할 것이니 Seconde Term은 버린다</b>  

= −∫〖𝑝(𝑦)∗log⁡(𝑞_𝑥 (𝑦))𝑑𝑦〗    

= −∑ ∫〖𝑝(𝑦)∗log⁡〖(𝑓_𝑖 (𝑥))/𝑤_𝑖 〗𝑑𝑦〗  
  
<b style="text-align: center;">우리의 아웃풋 분포 Q는 히스토그램이므로 integral이 아닌 시그마 형태로 바꿀 수 있다</b>  

= −∑log⁡〖(𝑓_𝑖 (𝑥))/𝑤_𝑖 〗*(𝐹(𝑙_𝑖 + 𝑤_𝑖)− 𝐹(𝑙_𝑖))    
  
<b style="text-align: center;">우리의 타겟 분포 P는 연속이기 때문에 integral 속의 p를 cdf를 사용하여 표현한다</b>  

≈ −∑𝑝_𝑖 *log(𝑓_𝑖 (𝑥))  
  
<b style="text-align: center;">히스토그램 bin의 width인 w는 상수이므로 버리고나면 히스토그램 로스는 k개의 이산적인 entropy 꼴이 된다</b>  

<br/>  

이렇게 우리의 로스를 이해하고 나면, "True Distribution" P를 무엇으로 믿을것인가에 대한 문제만 남는다. 베이지안 추론에서 Prior Probability를 적절히 설정하는 것이 중요하듯 좋은 P를 설정하는 것은 중요한데, 논문에서는 Truncated Normal Distribution / Dirac delta Distribution을 기본적으로 제안하고 Teacher Model이 존재하는 Classfication Task라면 soft label을 True P로 가정한다. 정규 분포를 사용하지 않고 Truncated 분포를 사용하는 이유는, 히스토그램인 Q와 구간의 범위를 맞추기 위함이다. 이렇듯, 다양한 P를 사용할 수 있어 상황에 따른 풍부한 해석이 가능하다고 저자는 말하고 있는데, 아쉽게도 논문의 모든 실험에서 Truncated Normal Distribution이 가장 좋은 성적을 보여 저자가 말한 상황이 무엇인지는 알 수 없었다.  

<br/>

<br/>
<img src="/assets/img/pr/hs/hstwo.jpg">  
<br>  

위와 같이 Truncated 정규 분포의 확률 값 p(i)를 구할 수 있는데, k개 구간의 확률을 한 번만 미리 구해놓으면, 우리의 f가 내놓는 k길이의 density ouput과의 연산을 반복해서 편하게 수행할 수 있다. 이것이 바로 저자가 Easy to Compute라고 주장한 이유. 물론 단순히 mse를 계산하는 것보다야 훨씬 코스트가 크겠지만, 기존의 다른 방법론에서 사용되던 연속 분포 두 개의 divergence를 계산하는 것보다야 훨씬 빠르다고 볼 수 있다. 또한 해당 로스의 가장 큰 장점은 "빠르게 수렴"하는 특성에 있는데, stochastic한 학습에서 빠르게 수렴하는 것이 일반화에 큰 도움이 되고, Loss의 Upper Bound가 작을수록 빠르게 수렴한다는 것이 15년도 논문에서 밝혀졌다고 하니 다음에 읽어봐야겠다. 하여튼 해당 로스가 "빠르게 수렴" 할 것이라는 추론은 아래의 수학적 논리를 통해 증명할 수 있다.  
<br/>

<br/>
<img src="/assets/img/pr/hs/hsthree.jpg">  
<br/>

<br/> 
Appendix까지 추가할 것이었으면 조금 더 친절하게 증명해줘도 되지 않았을까라는 생각이 들지만, 분수꼴의 미분과 편미분을 할 수 있다면 천천히 읽다보면 금방 이해할 수 있다. 결론만 말하자면 HS의 Gradient는 ∑p(i) - f(x_i)의 배수를 Upper Bound로 갖고, 같은 방식으로 MSE loss의 Upper bound를 계산하면 f(x) - y라는 것이다. HS의 것은 커봤자 1을 넘을 수 없지만 MSE의 로스는 매우매우 커질 수 있고, 이것은 빠르게 수렴하는데 방해가 될 것이라는 것이 저자의 주요 논지이다.  
<br/>

<br/> 

<img src="/assets/img/pr/hs/hsfour.jpg">  
<br/>

위의 실험이 그것을 증명하는데, 맨 왼쪽의 그래프를 통해 Truncated Normal Distribtion을 사용한 HS가 여러 비교군에 비해 빠르고 안정되게 학습되는 것을 볼 수 있다. 또한 옆의 오른쪽 두 그래프를 통해서는 빠르고 안정적일 뿐 아니라 더 정확하다는 것도 보여주고 있다.  
<br/>

<br/>
논문을 읽으며 매우 흥미로웠기 때문에 많은 부분에 활용되지 않았을까 생각했지만 논문의 신선함에 비해서는 Cited된 수가 너무 적어 의외였다. 최근에 이와 비슷한 Distributional Loss를 활용하여 시계열 모델을 적합하는 논문이 나왔다고 하여 다음에 리뷰해보겠다.  
<br/>

[(Paper URL)Improving regression performance with distributional losses](https://arxiv.org/pdf/1806.04613.pdf)  
